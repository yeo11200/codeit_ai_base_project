import torch
import torchvision.transforms as T
from torchvision.ops import nms
from PIL import Image
import os
import json
from tqdm import tqdm
import numpy as np
import cv2
import pandas as pd
import shutil
import xml.etree.ElementTree as ET

# model=get_model() í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ ë¬¸ì œìˆë˜ 850ì¥ì˜ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ê³  ì˜ˆì¸¡ê°’ì„ ìƒˆë¡œ ë¼ë²¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ëª¨ë¸ ì…ë ¥ í•„ìˆ˜
# device = 'cuda'

# í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ ë¼ë²¨ ë°ì´í„° ë§Œë“¤ê¸°(ì„¸ë¯¸ì˜¤í†  ë¼ë²¨ë§)
def predict_and_create_review_json(
    problematic_json_path, 
    source_images_folder, 
    model, 
    device, 
    output_json_path,
    score_threshold=0.2,  # ë‚®ì€ ì ìˆ˜ë„ ì¼ë‹¨ íƒì§€í•˜ë„ë¡ ì„¤ì •
    iou_threshold=0.3    # NMSë¥¼ ìœ„í•œ ê²¹ì¹¨ ê¸°ì¤€
):
    """
    [v3] ëª¨ë¸ ì˜ˆì¸¡ í›„ NMSë¥¼ ì ìš©í•˜ê³ , 'score'ê¹Œì§€ í•¨ê»˜ COCO JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("--- ğŸ¤– Step 1: ëª¨ë¸ ì´ˆë²Œ ë¼ë²¨ë§ (NMS + Score ì €ì¥) ì‹œì‘ ---")
    model.eval()
    
    with open(problematic_json_path, 'r', encoding='utf-8') as f:
        problematic_data = json.load(f)
        
    try:
        cat_id_to_label_map = {cat['id']: i + 1 for i, cat in enumerate(problematic_data['categories'])}
        label_to_cat_id_map = {v: k for k, v in cat_id_to_label_map.items()}
        print("-> ì›ë³¸ ì¹´í…Œê³ ë¦¬ ID <-> ëª¨ë¸ ë¼ë²¨ ë§¤í•‘ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì¹´í…Œê³ ë¦¬ ë§µ ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        return

    review_coco = {
        "images": problematic_data['images'],
        "annotations": [],
        "categories": problematic_data['categories']
    }
    
    transform = T.ToTensor()
    annotation_id_counter = 1
    
    for img_info in tqdm(problematic_data['images'], desc="ëª¨ë¸ ì˜ˆì¸¡ ì¤‘"):
        img_path = os.path.join(source_images_folder, img_info['file_name'])
        if not os.path.exists(img_path): continue

        image = Image.open(img_path).convert('RGB')
        image_tensor = transform(image).to(device).unsqueeze(0)
        
        with torch.no_grad():
            prediction = model(image_tensor)[0]

        # 1. score_thresholdë¥¼ ë„˜ëŠ” ë°•ìŠ¤ë§Œ ë¨¼ì € ê±°ë¦…ë‹ˆë‹¤.
        keep_by_score = prediction['scores'] > score_threshold
        high_score_boxes = prediction['boxes'][keep_by_score]
        high_score_labels = prediction['labels'][keep_by_score]
        high_score_scores = prediction['scores'][keep_by_score]

        # 2. NMSë¥¼ ì ìš©í•˜ì—¬ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        keep_by_nms = nms(high_score_boxes, high_score_scores, iou_threshold)
        
        final_boxes = high_score_boxes[keep_by_nms]
        final_labels = high_score_labels[keep_by_nms]
        final_scores = high_score_scores[keep_by_nms] # ğŸ‘ˆ NMSë¥¼ í†µê³¼í•œ ìµœì¢… ì ìˆ˜ë“¤

        for box, label, score in zip(final_boxes, final_labels, final_scores):
            box_np = box.cpu().numpy()
            x, y, xmax, ymax = box_np
            w, h = xmax - x, ymax - y
            original_cat_id = label_to_cat_id_map.get(int(label.cpu()), -1)

            new_ann = {
                "id": annotation_id_counter,
                "image_id": img_info['id'],
                "category_id": original_cat_id,
                "bbox": [int(x), int(y), int(w), int(h)],
                "area": float(w * h),
                "iscrowd": 0,
                "score": float(score.cpu().numpy()) # âœ… ì ìˆ˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
            }
            review_coco['annotations'].append(new_ann)
            annotation_id_counter += 1

    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(review_coco, f, ensure_ascii=False, indent=4)
        
    print(f"âœ… ì´ˆë²Œ ë¼ë²¨ë§ (NMS + Score ì €ì¥) ì™„ë£Œ! '{output_json_path}' íŒŒì¼ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


# ì§ì ‘ ê²€ìˆ˜ìš© ë°ì´í„° ì‹œê°í™”(ì´ë¯¸ì§€ íŒŒì¼í™”)
def visualize_all_predictions_with_score(
    review_json_path, 
    source_images_folder, 
    output_folder
):
    """
    [v3] 'ê²€í† ìš©' ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì „ì²´ë¥¼ ì‹œê°í™”í•˜ë©°, 'score'ë„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.
    """
    print(f"--- ğŸ–¼ï¸ 'ì´ˆë²Œ ë¼ë²¨ë§' ì „ì²´ ì‹œê°í™” (Score í‘œì‹œ) ì‹œì‘ ---")
    
    if not os.path.exists(review_json_path):
        print(f"ì˜¤ë¥˜: ê²€í† ìš© JSON íŒŒì¼ '{review_json_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    os.makedirs(output_folder, exist_ok=True)

    with open(review_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    try:
        cat_id_to_name_map = {
            cat['id']: cat['name'] for cat in data['categories']
        }
        print("-> ì›ë³¸ ì¹´í…Œê³ ë¦¬ IDì™€ ì´ë¦„ ë§¤í•‘ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì¹´í…Œê³ ë¦¬ ë§µ ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        return

    annos_by_img_id = {}
    for ann in data['annotations']:
        annos_by_img_id.setdefault(ann['image_id'], []).append(ann)
        
    print(f"ì´ {len(data['images'])}ê°œì˜ ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for img_info in tqdm(data['images'], desc="ì‹œê°í™” ì§„í–‰ ì¤‘"):
        img_path = os.path.join(source_images_folder, img_info['file_name'])
        if not os.path.exists(img_path): continue
            
        stream = open(img_path, "rb")
        bytes_data = bytearray(stream.read())
        numpyarray = np.asarray(bytes_data, dtype=np.uint8)
        image = cv2.imdecode(numpyarray, cv2.IMREAD_COLOR)
        stream.close()

        annotations = annos_by_img_id.get(img_info['id'], [])
        
        for ann in annotations:
            x, y, w, h = [int(v) for v in ann['bbox']]
            cat_id = ann['category_id']
            cat_name = cat_id_to_name_map.get(cat_id, f"ID:{cat_id}")

            score = ann.get('score', -1.0) # 1ë‹¨ê³„ì—ì„œ ì €ì¥í•œ scoreë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            
            if score >= 0:
                label_text = f"{cat_name}: {score:.2f}" # ì˜ˆ: "ë³´ë ¹ë¶€ìŠ¤íŒŒì •: 0.34"
            else:
                label_text = cat_name # scoreê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
            
            # ì ìˆ˜ì— ë”°ë¼ ë°•ìŠ¤ ìƒ‰ìƒ ë³€ê²½ (ì„ íƒ ì‚¬í•­)
            color = (0, 255, 0) # ê¸°ë³¸ê°’ (ë…¹ìƒ‰)
            if score < 0.5:
                color = (0, 255, 255) # ë‚®ì€ ì ìˆ˜ëŠ” ë…¸ë€ìƒ‰ìœ¼ë¡œ í‘œì‹œ
            
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            cv2.putText(image, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥
        output_path = os.path.join(output_folder, img_info['file_name'])
        extension = os.path.splitext(output_path)[1]
        result, encoded_img = cv2.imencode(extension, image)
        if result:
            with open(output_path, mode='w+b') as f:
                encoded_img.tofile(f)
                
    print(f"âœ… ê²€ìˆ˜ìš© ì‹œê°í™” ì™„ë£Œ! '{output_folder}' í´ë”ì—ì„œ 850ê°œì˜ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


# ìƒˆë¡œ ë¼ë²¨ë§ í•œ xmlíŒŒì¼ê³¼ ê¹¨ë—í•œ ì›ë³¸ ë°ì´í„°(639ì¥)ì˜ ì–´ë…¸í…Œì´ì…˜ ë³‘í•©

# xml íŒŒì¼ì„ coco ë°ì´í„° ì–‘ì‹ìœ¼ë¡œ ë³€í™˜
def parse_xml_to_coco_v2(
    xml_path, 
    name_to_cat_id_map, 
    categories_list, 
    next_new_cat_id
):
    """
    ë‹¨ì¼ XML íŒŒì¼ì„ ì½ê³ , ì‹ ê·œ ì¹´í…Œê³ ë¦¬ë¥¼ ë°œê²¬í•˜ë©´ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ë©° ë³€í™˜í•©ë‹ˆë‹¤.
    """
    tree = ET.parse(xml_path)
    root = tree.getroot()
    
    image_info = {
        "file_name": root.find('filename').text,
        "width": int(root.find('size/width').text),
        "height": int(root.find('size/height').text)
    }
    
    annotations = []
    for obj in root.findall('object'):
        label_name = obj.find('name').text
        
        # --- âœ… [í•µì‹¬ ìˆ˜ì •] ì‹ ê·œ ì¹´í…Œê³ ë¦¬ ê°ì§€ ë° ì¶”ê°€ ë¡œì§ ---
        if label_name not in name_to_cat_id_map:
            print(f"\n[ì‹ ê·œ ì¹´í…Œê³ ë¦¬ ê°ì§€] '{label_name}'ì„(ë¥¼) ëª©ë¡ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
            new_id = next_new_cat_id
            name_to_cat_id_map[label_name] = new_id # 'ë²ˆì—­ ì‚¬ì „' ì—…ë°ì´íŠ¸
            categories_list.append({ # 'ì¹´í…Œê³ ë¦¬ ëª©ë¡' ì—…ë°ì´íŠ¸
                "supercategory": "pill",
                "id": new_id,
                "name": label_name
            })
            next_new_cat_id += 1 # ë‹¤ìŒ ID ì¤€ë¹„
            
        category_id = name_to_cat_id_map[label_name]
        # --- [ìˆ˜ì • ë] ---
            
        bndbox = obj.find('bndbox')
        xmin = float(bndbox.find('xmin').text)
        ymin = float(bndbox.find('ymin').text)
        xmax = float(bndbox.find('xmax').text)
        ymax = float(bndbox.find('ymax').text)
        
        bbox = [xmin, ymin, xmax - xmin, ymax - ymin]
        
        annotations.append({
            "category_id": category_id,
            "bbox": bbox,
            "area": (xmax - xmin) * (ymax - ymin),
            "iscrowd": 0
        })
        
    # ë³€ê²½ëœ 'next_new_cat_id' ê°’ì„ ë°˜í™˜í•˜ì—¬ ë‹¤ìŒ XML íŒŒì‹±ì— ì‚¬ìš©
    return image_info, annotations, next_new_cat_id

# ì›ë³¸ ë°ì´í„°ì™€ì˜ ë³‘í•©
def package_dataset_with_xml_v2(
    clean_json_path,
    xml_folder_path,
    source_images_folder,
    output_folder
):
    """
    COCO(JSON)ì™€ PASCAL VOC(XML)ë¥¼ ë³‘í•©í•˜ë©°,
    XMLì—ë§Œ ì¡´ì¬í•˜ëŠ” ì‹ ê·œ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    print("--- ğŸš€ ìµœì¢… ë°ì´í„°ì…‹ (v_clean) ì¬êµ¬ì„± ì‹œì‘ (ì‹ ê·œ ì¹´í…Œê³ ë¦¬ ê°ì§€) ---")
    
    # --- Step 1: í´ë” ì´ˆê¸°í™” ---
    final_images_dir = os.path.join(output_folder, 'images')
    if os.path.exists(output_folder): shutil.rmtree(output_folder)
    os.makedirs(final_images_dir)
    print(f"\n[Phase 1] '{output_folder}' í´ë”ë¥¼ ê¹¨ë—í•˜ê²Œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.")

    # --- Step 2: ê¹¨ë—í•œ ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° 'ë²ˆì—­ ì‚¬ì „' ìƒì„± ---
    with open(clean_json_path, 'r', encoding='utf-8') as f:
        clean_data = json.load(f)
    
    # [ìˆ˜ì •] ì´ ë³€ìˆ˜ë“¤ì´ parse_xml_to_coco_v2 í•¨ìˆ˜ì— ì˜í•´ 'ì§ì ‘' ìˆ˜ì •ë  ê²ƒì…ë‹ˆë‹¤.
    categories_list = clean_data['categories']
    name_to_cat_id_map = {cat['name']: cat['id'] for cat in categories_list}
    
    # [ìˆ˜ì •] ì‹ ê·œ ID í• ë‹¹ì„ ìœ„í•œ ê¸°ì¤€ì  ì„¤ì •
    try:
        max_cat_id = max(cat['id'] for cat in categories_list)
        next_new_cat_id = max_cat_id + 1
    except ValueError:
        next_new_cat_id = 1 # ì¹´í…Œê³ ë¦¬ê°€ ì•„ì˜ˆ ë¹„ì–´ìˆì„ ê²½ìš°
        
    print(f"-> ê¸°ì¡´ {len(categories_list)}ê°œ ì¹´í…Œê³ ë¦¬ ë¡œë“œ. ì‹ ê·œ IDëŠ” {next_new_cat_id}ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")

    # ìµœì¢… COCO ë°ì´í„° êµ¬ì¡° (ê¹¨ë—í•œ 639ì¥ì„ ê¸°ë³¸ìœ¼ë¡œ)
    final_coco = {
        "images": clean_data['images'],
        "annotations": clean_data['annotations'],
        "categories": categories_list # ğŸ‘ˆ ìˆ˜ì •ë  ìˆ˜ ìˆëŠ” 'categories_list'ë¥¼ ì—°ê²°
    }
    
    image_id_offset = max(img['id'] for img in clean_data['images']) + 1 if clean_data['images'] else 1
    ann_id_offset = max(ann['id'] for ann in clean_data['annotations']) + 1 if clean_data['annotations'] else 1

    # --- Step 3: XML ë³€í™˜ ë° ë³‘í•© (v2) ---
    print(f"\n[Phase 2] '{xml_folder_path}' í´ë”ì—ì„œ XML íŒŒì¼ì„ ë³€í™˜ ë° ë³‘í•©í•©ë‹ˆë‹¤...")
    xml_files = [f for f in os.listdir(xml_folder_path) if f.endswith('.xml')]
    newly_added_filenames = set()
    
    for xml_file in tqdm(xml_files, desc="XML ë³€í™˜ ì¤‘"):
        xml_path = os.path.join(xml_folder_path, xml_file)
        
        # [ìˆ˜ì •] v2 í•¨ìˆ˜ í˜¸ì¶œ
        new_image_info, new_annotations, next_new_cat_id = parse_xml_to_coco_v2(
            xml_path, 
            name_to_cat_id_map,  # 'ë²ˆì—­ ì‚¬ì „' (ìˆ˜ì • ê°€ëŠ¥)
            categories_list,     # 'ì¹´í…Œê³ ë¦¬ ëª©ë¡' (ìˆ˜ì • ê°€ëŠ¥)
            next_new_cat_id      # 'ë‹¤ìŒ ID'
        )
        
        new_image_id = image_id_offset
        new_image_info['id'] = new_image_id
        final_coco['images'].append(new_image_info)
        newly_added_filenames.add(new_image_info['file_name'])
        
        for ann in new_annotations:
            ann['id'] = ann_id_offset
            ann['image_id'] = new_image_id
            final_coco['annotations'].append(ann)
            ann_id_offset += 1
            
        image_id_offset += 1
        
    print(f"-> {len(xml_files)}ê°œì˜ XML íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë³‘í•©í–ˆìŠµë‹ˆë‹¤.")
    print(f"-> ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(final_coco['categories'])}ê°œ")

    # --- Step 4: ì´ë¯¸ì§€ íŒŒì¼ í†µí•© ---
    clean_filenames = {img['file_name'] for img in clean_data['images']}
    all_filenames = clean_filenames.union(newly_added_filenames)
    
    print(f"\n[Phase 3] ì´ {len(all_filenames)}ê°œì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³µì‚¬í•©ë‹ˆë‹¤...")
    for filename in tqdm(all_filenames, desc="ì „ì²´ ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬"):
        source_path = os.path.join(source_images_folder, filename)
        if os.path.exists(source_path):
            shutil.copy(source_path, final_images_dir)
        else:
            print(f"ê²½ê³ : ì›ë³¸ ì´ë¯¸ì§€ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    # --- Step 5: ìµœì¢… ì €ì¥ ---
    output_json_path = os.path.join(output_folder, 'final_clean_annotations.json')
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(final_coco, f, ensure_ascii=False, indent=4)
    
    print("\n--- âœ… ìµœì¢… (v_clean) ì¬êµ¬ì„± ì™„ë£Œ! ---")
    print(f"'{output_folder}'ì— ì´ {len(final_coco['images'])}ê°œì˜ ì´ë¯¸ì§€ì™€ {len(final_coco['annotations'])}ê°œì˜ ì–´ë…¸í…Œì´ì…˜ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ì›ë³¸ë°ì´í„° + ì‹ ê·œ ë¼ë²¨ë§ ë°ì´í„° ë³‘í•©
def merge_final_datasets(
    base_clean_json_path,    # ğŸ‘ˆ 100% ì‹ ë¢°í•˜ëŠ” 645ì¥ ë°ì´í„°
    review_json_path,      # ğŸ‘ˆ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ 844ì¥ ë°ì´í„°
    source_images_folder,  # ğŸ‘ˆ 1489ì¥ ì›ë³¸ ì´ë¯¸ì§€ í´ë”
    output_folder
):
    """
    100% ì‹ ë¢°í•˜ëŠ” 'ë² ì´ìŠ¤' ë°ì´í„°ì…‹ê³¼,
    ë² ì´ìŠ¤ì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” 'ëª¨ë¸ ì˜ˆì¸¡' ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ì—¬
    ìµœì¢… 1489ì¥ ë°ì´í„°ì…‹ì„ íŒ¨í‚¤ì§•í•©ë‹ˆë‹¤.
    """
    print("--- ğŸš€ ìµœì¢… ë°ì´í„°ì…‹ (v_final) ì¬êµ¬ì„± ì‹œì‘ (Clean 645 + Predicted 844) ---")
    
    # --- Step 1: í´ë” ì´ˆê¸°í™” ---
    final_images_dir = os.path.join(output_folder, 'images')
    if os.path.exists(output_folder):
        shutil.rmtree(output_folder)
    os.makedirs(final_images_dir)
    print(f"\n[Phase 1] '{output_folder}' í´ë”ë¥¼ ê¹¨ë—í•˜ê²Œ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.")

    # --- Step 2: [ì†ŒìŠ¤ A] 100% ì‹ ë¢°í•˜ëŠ” ë² ì´ìŠ¤(645ì¥) ë¡œë“œ ---
    with open(base_clean_json_path, 'r', encoding='utf-8') as f:
        base_data = json.load(f)
    
    # ë² ì´ìŠ¤ì— í¬í•¨ëœ íŒŒì¼ëª… ëª©ë¡ (ì´ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í•„í„°ë§)
    base_filenames = {img['file_name'] for img in base_data['images']}
    print(f"-> ë² ì´ìŠ¤ ë°ì´í„° {len(base_filenames)}ì¥ ë¡œë“œ ì™„ë£Œ.")

    # ìµœì¢… COCO ë°ì´í„° êµ¬ì¡° (ë² ì´ìŠ¤ë¥¼ ê¸°ë³¸ìœ¼ë¡œ)
    final_coco = {
        "images": base_data['images'],
        "annotations": base_data['annotations'],
        "categories": base_data['categories']
    }
    
    # ID ì¬ì„¤ì •ì„ ìœ„í•œ ê¸°ì¤€ì (offset) ê³„ì‚°
    image_id_offset = max(img['id'] for img in base_data['images']) + 1 if base_data['images'] else 1
    ann_id_offset = max(ann['id'] for ann in base_data['annotations']) + 1 if base_data['annotations'] else 1

    # --- Step 3: [ì†ŒìŠ¤ B] ëª¨ë¸ ì˜ˆì¸¡(844ì¥) í•„í„°ë§ ë° ë³‘í•© ---
    print(f"\n[Phase 2] '{review_json_path}'ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ í•„í„°ë§ ë° ë³‘í•©í•©ë‹ˆë‹¤...")
    with open(review_json_path, 'r', encoding='utf-8') as f:
        review_data = json.load(f)

    # [í•µì‹¬] ë² ì´ìŠ¤(645ì¥)ì— í¬í•¨ë˜ì§€ ì•Šì€ 844ê°œì˜ ì´ë¯¸ì§€ë§Œ í•„í„°ë§
    images_to_add = [
        img for img in review_data['images'] 
        if img['file_name'] not in base_filenames
    ]
    image_ids_to_add = {img['id'] for img in images_to_add}
    
    # 844ê°œ ì´ë¯¸ì§€ì— ì—°ê²°ëœ ì–´ë…¸í…Œì´ì…˜ë§Œ í•„í„°ë§
    annotations_to_add = [
        ann for ann in review_data['annotations'] 
        if ann['image_id'] in image_ids_to_add
    ]
    print(f"-> {len(images_to_add)}ê°œì˜ ìˆœìˆ˜ ì˜ˆì¸¡ ë°ì´í„°(850 - {850 - len(images_to_add)})ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.")

    # ID ì¬ì„¤ì • ë° ìµœì¢… ë³‘í•©
    review_id_map = {img['id']: img['id'] + image_id_offset for img in images_to_add}
    
    for img in images_to_add:
        img['id'] = review_id_map[img['id']]
        final_coco['images'].append(img)

    for ann in annotations_to_add:
        ann['id'] += ann_id_offset
        ann['image_id'] = review_id_map[ann['image_id']]
        final_coco['annotations'].append(ann)
        ann_id_offset += 1
    
    image_id_offset += len(images_to_add)

    # --- Step 4: ì´ë¯¸ì§€ íŒŒì¼ í†µí•© ---
    all_filenames = {img['file_name'] for img in final_coco['images']}
    print(f"\n[Phase 3] ì´ {len(all_filenames)}ê°œì˜ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³µì‚¬í•©ë‹ˆë‹¤...")
    if len(all_filenames) != 1489:
        print(f"ê²½ê³ : ìµœì¢… ì´ë¯¸ì§€ ìˆ˜ê°€ 1489ì¥ì´ ì•„ë‹™ë‹ˆë‹¤! (í˜„ì¬: {len(all_filenames)}ì¥)")
        
    for filename in tqdm(all_filenames, desc="ì „ì²´ ì›ë³¸ ì´ë¯¸ì§€ ë³µì‚¬"):
        source_path = os.path.join(source_images_folder, filename)
        if os.path.exists(source_path):
            shutil.copy(source_path, final_images_dir)
        else:
            print(f"ê²½ê³ : ì›ë³¸ ì´ë¯¸ì§€ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

    # --- Step 5: ìµœì¢… ì €ì¥ ---
    output_json_path = os.path.join(output_folder, 'final_1489_annotations.json')
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(final_coco, f, ensure_ascii=False, indent=4)
    
    print("\n--- âœ… ìµœì¢… (v_final) ì¬êµ¬ì„± ì™„ë£Œ! ---")
    print(f"'{output_folder}'ì— ì´ {len(final_coco['images'])}ê°œì˜ ì´ë¯¸ì§€ì™€ {len(final_coco['annotations'])}ê°œì˜ ì–´ë…¸í…Œì´ì…˜ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(final_coco['categories'])}ê°œ")


# ë°ì´í„°ì…‹ ë¬´ê²°ì„± ê²€ì‚¬
def get_expected_count_from_filename(filename):
    """
    íŒŒì¼ëª…ì˜ ì•½í’ˆ ì½”ë“œ ë¶€ë¶„ì„ ë¶„ì„í•˜ì—¬ ê¸°ëŒ€ë˜ëŠ” ì•Œì•½ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: 'K-001-002-003_...' -> 3
    """
    try:
        # íŒŒì¼ëª…ì—ì„œ ì²«ë²ˆì§¸ '_' ì•ë¶€ë¶„ì„ ì¶”ì¶œ (ì˜ˆ: 'K-001900-010224-016551')
        key_part = filename.split('_')[0]
        # '-'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œìˆ˜ë¥¼ ì…ˆ
        num_parts = len(key_part.split('-'))
        # 'K' ë¶€ë¶„ì„ ì œì™¸í•œ ê°œìˆ˜ê°€ ì•Œì•½ì˜ ê°œìˆ˜
        expected_count = num_parts - 1
        return expected_count
    except Exception:
        # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ íŒŒì¼ëª…ì¼ ê²½ìš° 0ì„ ë°˜í™˜
        return 0

def verify_annotation_counts_by_filename(merged_json_path):
    """
    ë³‘í•©ëœ COCO íŒŒì¼ì„ ë¶„ì„í•˜ì—¬, íŒŒì¼ëª… ê·œì¹™ì— ë”°ë¥¸ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(merged_json_path):
        print(f"ì˜¤ë¥˜: '{merged_json_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    try:
        with open(merged_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print(f"ì˜¤ë¥˜: '{merged_json_path}' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return


    images_df = pd.DataFrame(data.get('images', []))
    annotations_df = pd.DataFrame(data.get('annotations', []))

    if images_df.empty:
        print("ì˜¤ë¥˜: JSON íŒŒì¼ì— ì´ë¯¸ì§€ ì •ë³´('images' key)ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    if annotations_df.empty:
        print("ê²½ê³ : JSON íŒŒì¼ì— ì–´ë…¸í…Œì´ì…˜ ì •ë³´('annotations' key)ê°€ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì‹¤ì œê°’ì´ 0ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
        # ì–´ë…¸í…Œì´ì…˜ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ DataFrame ìƒì„±
        actual_counts = pd.DataFrame(columns=['id', 'actual_count'])
    else:
        # 1. ê° ì´ë¯¸ì§€ë³„ ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ ê³„ì‚°
        actual_counts = annotations_df['image_id'].value_counts().reset_index()
        actual_counts.columns = ['id', 'actual_count']

    # 2. ì´ë¯¸ì§€ ì •ë³´ì™€ ì‹¤ì œ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ ë³‘í•©
    analysis_df = pd.merge(images_df, actual_counts, on='id', how='left')
    analysis_df['actual_count'] = analysis_df['actual_count'].fillna(0).astype(int)

    # 3. íŒŒì¼ëª…ì—ì„œ ê¸°ëŒ€ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ ì¶”ì¶œ
    analysis_df['expected_count'] = analysis_df['file_name'].apply(get_expected_count_from_filename)

    # 4. ê¸°ëŒ€ê°’ê³¼ ì‹¤ì œê°’ì´ ë‹¤ë¥¸ ê²½ìš° í•„í„°ë§
    # (ë‹¨, íŒŒì¼ëª… ê·œì¹™ì´ ì—†ëŠ” ì´ë¯¸ì§€(expected_count=0)ëŠ” ê²€ì¦ì—ì„œ ì œì™¸)
    mismatched_df = analysis_df[
        (analysis_df['actual_count'] != analysis_df['expected_count']) &
        (analysis_df['expected_count'] > 0)
    ].copy() # SettingWithCopyWarning ë°©ì§€ë¥¼ ìœ„í•´ .copy() ì‚¬ìš©

    # --- ê²°ê³¼ ë¦¬í¬íŠ¸ ---
    total_images_to_check = len(analysis_df[analysis_df['expected_count'] > 0])
    total_mismatched = len(mismatched_df)
    total_correct = total_images_to_check - total_mismatched

    print("--- ğŸ”¬ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼ ---")
    if total_images_to_check == 0:
        print("ê²€ì¦í•  ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ëª… ê·œì¹™ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    print(f"ì´ {total_images_to_check}ê°œ ì´ë¯¸ì§€ë¥¼ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.")
    print(f"âœ… {total_correct}ê°œ ì´ë¯¸ì§€ëŠ” íŒŒì¼ëª…ê³¼ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.")

    if not mismatched_df.empty:
        print(f"\nğŸš¨ {total_mismatched}ê°œ ì´ë¯¸ì§€ì—ì„œ ê°œìˆ˜ ë¶ˆì¼ì¹˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
        # ë³´ê¸° í¸í•˜ê²Œ ì—´ ì´ë¦„ ë³€ê²½í•˜ì—¬ ì¶œë ¥
        mismatched_df.rename(columns={
            'file_name': 'íŒŒì¼ëª…',
            'expected_count': 'ê¸°ëŒ€ ê°œìˆ˜',
            'actual_count': 'ì‹¤ì œ ê°œìˆ˜'
        }, inplace=True)
        report = mismatched_df[['íŒŒì¼ëª…', 'ê¸°ëŒ€ ê°œìˆ˜', 'ì‹¤ì œ ê°œìˆ˜']]
        print(report.to_string(index=False))
    else:
        print("\nğŸ‰ ì™„ë²½í•©ë‹ˆë‹¤! ëª¨ë“  ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜ ê°œìˆ˜ê°€ íŒŒì¼ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤.")
    print("-" * 50)



#----------ì‹¤í–‰ ì½”ë“œ-----------

if __name__ == '__main__':
    # ì‚¬ì „ í•™ìŠµ ëª¨ë¸ë¡œ ë¼ë²¨ ë°ì´í„° ì˜ˆì¸¡
    predict_and_create_review_json(
    problematic_json_path='problematic_annotations.json', # 850ì¥ì˜ë¬¸ì œ ìˆëŠ” ë°ì´í„°ì˜ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼
    source_images_folder='data/train_images', # ì›ë³¸ í•™ìŠµ ì´ë¯¸ì§€ í´ë”
    model=model, #ì‚¬ì „ í•™ìŠµì‹œí‚¨ ëª¨ë¸ ë¡œë“œ
    device=device, #cuda or cpu
    output_json_path='annotations_for_review.json' #ê²°ê³¼ë¬¼ì„ jsonìœ¼ë¡œ ì €ì¥
    )

    # ê²€ìˆ˜ë¥¼ ìœ„í•œ ì‹œê°í™”+ì´ë¯¸ì§€ íŒŒì¼í™”
    visualize_all_predictions_with_score(
    'annotations_for_review.json', # ì„¸ë¯¸ì˜¤í†  ë¼ë²¨ë§ì„ ìœ„í•œ, ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
    'data/train_images', # ì›ë³¸ í•™ìŠµ ì´ë¯¸ì§€ í´ë”
    'review_visualizations_1' #ì‹ ê·œ í´ë”
    )

    # ì§ì ‘ ë¼ë²¨ë§í•œ ë°ì´í„°ì™€ ì›ë³¸ ê¹¨ë—í•œ ë°ì´í„° ë³‘í•©(639ì¥ + 6ì¥)
    package_dataset_with_xml_v2(
    clean_json_path='cleaned_annotations.json', # 639ì¥ì˜ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼
    xml_folder_path='new_xml_labels', # ìƒˆë¡œ ì§ì ‘ ë§Œë“  ë¼ë²¨ë°ì´í„° 6ì¥ í´ë”
    source_images_folder='data/train_images', #ì›ë³¸ í•™ìŠµì´ë¯¸ì§€ í´ë”
    output_folder='final_dataset_v_clean'
    )

    # ê¹¨ë—í•œ ë°ì´í„°(639+6ì¥)ì™€ ì‹ ê·œ ë¼ë²¨ë§ ë°ì´í„°(844ì¥) ìµœì¢… ë³‘í•©
    merge_final_datasets(
    base_clean_json_path='final_dataset_v_clean/final_clean_annotations.json', # ğŸ‘ˆ 1ìˆœìœ„ (645ì¥)
    review_json_path='annotations_for_review.json',                       # ğŸ‘ˆ 2ìˆœìœ„ (850ì¥)
    source_images_folder='data/train_images',                             # ğŸ‘ˆ 1489ì¥ ì›ë³¸ í´ë”
    output_folder='final_dataset_1489'                                    # ğŸ‘ˆ ìµœì¢… ê²°ê³¼ë¬¼ í´ë”
    )

    # ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬
    json_file_path = 'final_dataset_1489/final_1489_annotations.json' 
    verify_annotation_counts_by_filename(json_file_path)